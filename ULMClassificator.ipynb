{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing ULMFit with tensorflow - Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is born to implement ULMfit model. This is a mixture of different papers: the first and the second are from Merity et al 2017 https://arxiv.org/pdf/1708.02182.pdf which inspired the ULMfit lm phase, the second is https://arxiv.org/pdf/1711.03953.pdf which provides us with a optimization on the softmax layer which greatly boost the lm performance, and last but not the least the ULMfit paper by Ruder et al 2018 https://arxiv.org/pdf/1801.06146.pdf . \n",
    "In this notebook we implement the finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrea\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from LMNets.models import LanguageModelAWD, LanguageModelMoS\n",
    "from LMNets.losses import lm_loss_sparse, log_scalar, cross_entropy_w_softmax\n",
    "from LMNets.optimize import minimize_w_clipping\n",
    "from LMNets.data import penn_treebank, wikitext, iterator\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_score(model, x, y, seq_len):\n",
    "    y_, _ =  model.forward(x, seq_len)\n",
    "    return tf.pow(2,tf.losses.sparse_softmax_cross_entropy(y, logits=y_))\n",
    "\n",
    "def compute_token_score(model, tokens, bs, bptt):\n",
    "    scores = []\n",
    "    for x, y, seq_len in iterator.get_bptt_batch_iterator(tokens, bs, bptt):\n",
    "        scores.append(batch_score(model, x, y, seq_len))\n",
    "    return np.mean(scores)\n",
    "\n",
    "def evaluate_using_weighted_f1(model, X_test, y_test, X_val, y_val,\n",
    "                               batch_size):\n",
    "    \n",
    "    y_pred_test = np.array(model.predict(X_test, batch_size=batch_size))\n",
    "    y_pred_val = np.array(model.predict(X_val, batch_size=batch_size))\n",
    "\n",
    "    f1_test, _ = find_f1_threshold(y_val, y_pred_val, y_test, y_pred_test,\n",
    "                                   average='weighted')\n",
    "    return f1_test\n",
    "\n",
    "def find_f1_threshold(y_val, y_pred_val, y_test, y_pred_test,\n",
    "                      average='binary'):\n",
    "\n",
    "    thresholds = np.arange(0.01, 0.5, step=0.01)\n",
    "    f1_scores = []\n",
    "\n",
    "    for t in thresholds:\n",
    "        y_pred_val_ind = (y_pred_val > t)\n",
    "        f1_val = f1_score(y_val, y_pred_val_ind, average=average)\n",
    "        f1_scores.append(f1_val)\n",
    "\n",
    "    best_t = thresholds[np.argmax(f1_scores)]\n",
    "    y_pred_ind = (y_pred_test > best_t)\n",
    "    f1_test = f1_score(y_test, y_pred_ind, average=average)\n",
    "    return f1_test, best_t\n",
    "\n",
    "def my_score_fun(y_, y):\n",
    "    args=np.argmax(y_, axis=1)\n",
    "    y_ = np.zeros(y.shape)\n",
    "    y_[range(y_.shape[0]),args] = 1\n",
    "    return accuracy_score(y, y_)\n",
    "\n",
    "def compute_score(model, x_val, y_true, n_classes, bs, score_fun):\n",
    "    scores = []\n",
    "    \n",
    "    for x, y, seq_len in iterator.get_batch_iterator(x_val, y_true, n_classes, bs):\n",
    "        scores.append(score_fun(model.forward(x, seq_len), y))\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseDropBN:\n",
    "    \n",
    "    def __init__(self, shape, dropout):\n",
    "        self.trainable = True\n",
    "        self.shape = shape\n",
    "        self.dropout_rate = dropout\n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        self.dense = tf.Variable(tf.random_normal((self.shape)))\n",
    "        self._trainable_weights = self.dense\n",
    "        \n",
    "    def forward(self, input_, training=True):\n",
    "        dropout_dense = tf.layers.dropout(self.dense, self.dropout_rate)\n",
    "        batch_norm_activ = tf.layers.batch_normalization(tf.matmul(input_, dropout_dense), \n",
    "                                                         training=training)\n",
    "        return tf.nn.relu(batch_norm_activ)\n",
    "    \n",
    "    def get_trainable_weights(self):\n",
    "        return [self._trainable_weights] if self.trainable else []\n",
    "    \n",
    "class TransferModel:\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, training=True):\n",
    "        self.hidden_dim = 256\n",
    "        self.dropout_rate = 0.4\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.training = training\n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        self.dense1 = DenseDropBN((self.input_dim, self.hidden_dim), self.dropout_rate)\n",
    "        self.dense2 = DenseDropBN((self.hidden_dim, self.output_dim), self.dropout_rate)\n",
    "        \n",
    "        self.layers = [self.dense1, self.dense2]\n",
    "        \n",
    "    def forward(self, input_):\n",
    "        h_output = self.dense1.forward(input_, training=self.training)\n",
    "        result = self.dense2.forward(h_output, training=self.training)\n",
    "        return result\n",
    "\n",
    "\n",
    "class UniversalLMClassifier:\n",
    "    \n",
    "    def __init__(self, language_model, transfer_model):\n",
    "        self.lm = language_model\n",
    "        self.tf_model = transfer_model\n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        self.layers = self.lm.layers[:-1] + self.tf_model.layers\n",
    "        self.ckp = tf.train.Checkpoint(**dict([(str(i),var) for i, var in enumerate(self.get_trainable_weights())]))\n",
    "\n",
    "    def forward(self, X, seq_len):\n",
    "        output = self.lm.forward_last(X, seq_len)\n",
    "        return self.tf_model.forward(output)\n",
    "    \n",
    "    def slanted_t_lr(self, t, T, cut_frac, ratio, n_max):\n",
    "        p = 0\n",
    "        cut = T * cut_frac\n",
    "        if t < cut:\n",
    "            p = t/cut\n",
    "        else:\n",
    "            p = 1 - (t - cut) / (cut * (1 / (cut_frac - 1))) \n",
    "        lr = n_max * (1 + p*(ratio - 1)) / ratio\n",
    "        return lr\n",
    "    \n",
    "    def discr_finetuning(self, gradients, layers):\n",
    "        var_count = len(gradients)\n",
    "        layer_index = 0\n",
    "        for level_layer, layer in enumerate(layers[::-1]):\n",
    "            trainable_weights = layer.get_trainable_weights()\n",
    "            for _ in trainable_weights:\n",
    "                if level_layer > 0:\n",
    "                    gradients[var_count - 1 - layer_index] /= (level_layer*2.6)\n",
    "                layer_index += 1\n",
    "    \n",
    "    def fine_tune(self,\n",
    "                  train_tokens, \n",
    "                  val_tokens, \n",
    "                  epochs, \n",
    "                  loss, \n",
    "                  logging=False,\n",
    "                  log_dir=\"./finetune_log/\",\n",
    "                  ckpt_dir=\"./finetune_ckpts\",\n",
    "                  batch_size=32, \n",
    "                  val_bs=32,\n",
    "                  cut_frac=0.1, \n",
    "                  ratio=32, \n",
    "                  n_max=0.01, \n",
    "                  bptt=10):\n",
    "        \n",
    "        if logging:\n",
    "            summary_writer = tf.contrib.summary.create_file_writer(log_dir, flush_millis=10000)\n",
    "            summary_writer.set_as_default()\n",
    "            global_step = tf.train.get_or_create_global_step()\n",
    "        \n",
    "        iteration = 0\n",
    "        T_estimate = epochs * int(len(train_tokens) / (bptt*1.5*batch_size))\n",
    "        current_val_score = compute_token_score(self.lm, val_tokens, val_bs, bptt)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for x_train, y_train, seq_len in iterator.get_bptt_batch_iterator(train_tokens, batch_size, bptt):\n",
    "                \n",
    "                if logging:\n",
    "                    global_step.assign_add(1)\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                    loss_ = loss(self.lm, \n",
    "                                x_train, \n",
    "                                y_train, \n",
    "                                seq_len, \n",
    "                                logging=logging,\n",
    "                                iteration=iteration)\n",
    "                trainable_weights = self.lm.get_trainable_weights()\n",
    "                gradients = tape.gradient(loss_, trainable_weights)\n",
    "                # Apply Slanted Triangular learning rate\n",
    "                gradients = [grad * self.slanted_t_lr(iteration, T_estimate, cut_frac, ratio, n_max)\n",
    "                             for grad in gradients]\n",
    "                # Apply discriminative finetuning\n",
    "                self.discr_finetuning(gradients, self.lm.layers)\n",
    "                # Apply gradient clipping\n",
    "                gradients = [tf.clip_by_norm(grad, clip_norm=0.25) for grad in gradients]\n",
    "                # Update weights\n",
    "                self.lm.backward(trainable_weights, gradients)\n",
    "            \n",
    "            val_score = compute_token_score(self.lm, val_tokens, val_bs)\n",
    "            \n",
    "            if val_score < current_val_score:\n",
    "                self.lm.save_model(ckpt=ckpt_dir)\n",
    "\n",
    "    \n",
    "    def train(self,\n",
    "              x_train, \n",
    "              y_train, \n",
    "              x_val, \n",
    "              y_val, \n",
    "              loss, \n",
    "              epochs,\n",
    "              score_fun,\n",
    "              logging=False,\n",
    "              log_dir=\"./ulm_log/\",\n",
    "              ckpt_dir=\"./ulm_ckpt/\",\n",
    "              batch_size=32, \n",
    "              val_bs=32,\n",
    "              cut_frac=0.1, \n",
    "              ratio=32, \n",
    "              n_max=0.01):\n",
    "        \n",
    "        if logging:\n",
    "            summary_writer = tf.contrib.summary.create_file_writer(log_dir, flush_millis=10000)\n",
    "            summary_writer.set_as_default()\n",
    "            global_step = tf.train.get_or_create_global_step()\n",
    "            \n",
    "        iteration = 0\n",
    "        n_classes = y_train.shape[1]\n",
    "        T = epochs * ceil(x_train.shape[0] / batch_size)\n",
    "        current_val_score = compute_score(self, x_val, y_val, n_classes, val_bs, score_fun)\n",
    "        for epoch in range(epochs):\n",
    "            for x_train, y_train, seq_len in iterator.get_batch_iterator(x_train, y_train, n_classes, batch_size):\n",
    "                \n",
    "                if logging:\n",
    "                    global_step.assign_add(1)\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                    loss_ = loss(self, \n",
    "                                x_train, \n",
    "                                y_train, \n",
    "                                seq_len, \n",
    "                                logging=logging,\n",
    "                                iteration=iteration)\n",
    "                # Get unfrozen weights\n",
    "                trainable_weights = [weight for layer in self.layers[-epoch-1:] for weight in layer.get_trainable_weights()]\n",
    "                gradients = tape.gradient(loss_, trainable_weights)\n",
    "                # Apply Slanted Triangular learning rate\n",
    "                gradients = [grad * self.slanted_t_lr(iteration, T, cut_frac, ratio, n_max)\n",
    "                             for grad in gradients]\n",
    "                # Apply discriminative finetuning on the unfrozer layers\n",
    "                self.discr_finetuning(gradients, self.layers[-epoch-1:])\n",
    "                # Apply gradient clipping\n",
    "                gradients = [tf.clip_by_norm(grad, clip_norm=0.25) for grad in gradients]\n",
    "                # Update weights\n",
    "                self.backward(trainable_weights, gradients)\n",
    "            \n",
    "            val_score = compute_score(self, x_val, y_val, n_classes, val_bs, score_fun)\n",
    "            \n",
    "            if val_score > current_val_score:\n",
    "                self.save_model(ckpt=ckpt_dir)\n",
    "    \n",
    "    def backward(self, weights, gradients):\n",
    "        for weight, grad in zip(weights, gradients):\n",
    "            weight.assign_sub(grad)\n",
    "            \n",
    "    def get_trainable_weights(self):\n",
    "        return [weight for layer in self.layers for weight in layer.get_trainable_weights()]\n",
    "    \n",
    "    def save_model(self, ckpt=\"./ulm_ckpt/\"):\n",
    "        self.ckp.save(ckpt)\n",
    "\n",
    "    def restore_model(self, ckpt=\"./ulm_ckpt/\"):\n",
    "        self.ckp.restore(tf.train.latest_checkpoint(ckpt))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy finetuning and training of the language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_voc = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_tr_tokens, p_val_tokens, p_test_tokens, p_vocab = penn_treebank.load_data(max_voc)\n",
    "voc_size = len(p_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "awd_asgd = LanguageModelAWD(voc_size)\n",
    "awd_asgd.restore_model()\n",
    "\n",
    "tf_model = TransferModel(awd_asgd.layers[-2].units, 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulm_clf = UniversalLMClassifier(awd_asgd, tf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ulm_clf.fine_tune(p_tr_tokens, p_val_tokens, 1, lm_loss_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "import string \n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "start_token = 26\n",
    "end_token = 27\n",
    "input_size = 32\n",
    "timesteps = 10\n",
    "\n",
    "def get_next_char(char):\n",
    "    next_id = char2id[char] + 1\n",
    "    next_id = 0 if next_id > 25 else next_id\n",
    "    return id2char[next_id]\n",
    "\n",
    "def get_next_chars_from_nparray(array):\n",
    "    return [get_next_char(char) for char in array]\n",
    "\n",
    "\n",
    "chars = list(string.ascii_lowercase)\n",
    "\n",
    "id2char = {start_token:\"\\t\", end_token:\"\\n\"}\n",
    "char2id = dict([(token,key) for key, token in id2char.items()])\n",
    "\n",
    "for i,char in enumerate(chars):\n",
    "    id2char[i] = char\n",
    "    char2id[char] = i\n",
    "\n",
    "enc_inputs = np.random.choice(chars, (input_size, timesteps))\n",
    "dec_inputs = np.array([get_next_chars_from_nparray(example) for example in enc_inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_x = np.array([char2id[c] for arr in enc_inputs for c in arr]).reshape((input_size,timesteps))\n",
    "lb.fit([c for c in string.ascii_letters[0:26]])\n",
    "my_y = lb.transform(dec_inputs[:,-1])\n",
    "seq_len = [timesteps]*input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulm_clf.train(my_x, my_y, my_x, my_y, cross_entropy_w_softmax, 100, my_score_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_token_score(awd_asgd, p_val_tokens, 256, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
